{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e24dd1-79df-424e-8741-59ddcc5f051e",
   "metadata": {},
   "source": [
    "### Author: Rafael de Oliveira MagalhÃ£es\n",
    "\n",
    "# BehaviorXplore "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6881b56-71da-457e-80b2-22f31a21754e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842861b5-bb6b-4544-b770-577220f7b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape, Conv2D, MaxPooling2D, SeparableConv2D, Lambda\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.metrics import Metric\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Add, Concatenate, Input, GlobalAveragePooling2D, Layer\n",
    "from keras import models, initializers\n",
    "from keras.models import Model\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, brier_score_loss\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from spektral.datasets import TUDataset\n",
    "from spektral.layers import GCNConv, GlobalSumPool, ChebConv\n",
    "from spektral.data import SingleLoader, BatchLoader\n",
    "from spektral.data import Graph\n",
    "from spektral.data import Dataset\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import f\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# Helper libraries\n",
    "\n",
    "from bokeh.io import show\n",
    "from bokeh.plotting import gmap\n",
    "from bokeh.models import GMapOptions\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from collections import deque\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import csv\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import math as m\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "import datetime as dt\n",
    "import re\n",
    "import time\n",
    "import gmaps as gm\n",
    "import seaborn as sns\n",
    "from statistics import mean\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "from numba import jit, cuda\n",
    "from sodapy import Socrata\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings as w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bfc17d-3484-4d4a-9746-fa98f1960f17",
   "metadata": {},
   "source": [
    "# Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256c0f69-2287-46f1-a30d-dafefefbd11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict(array: list) -> dict:\n",
    "    \"\"\"\n",
    "        Generate a dict from a array\n",
    "    \"\"\"\n",
    "    dictionary = {value: index for index, value in enumerate(array)}\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd7b09-0214-47f9-98b0-57a8c7bb419f",
   "metadata": {},
   "source": [
    "# Methods - Neural Network Input Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158190f6-9fac-472b-9855-d44dee85a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_short_time(temporal_series: np.array, index: int, s: int) -> tuple:\n",
    "    \"\"\"\n",
    "        Generate short time data in format of numpy array\n",
    "\n",
    "        Parameters:\n",
    "        - temporal_series: a numpy array that represents a temporal series\n",
    "        - index: The sample index\n",
    "        - s: interval between selected samples of the temporal series.\n",
    "             This value must be an integer representing the number of time series samples to be discarded\n",
    "\n",
    "        Returns:\n",
    "        A tuple that contains a numpy array of the short time data and the corresponding expected data\n",
    "    \"\"\"\n",
    "    expected_vector = temporal_series[:,index]\n",
    "    previous_data = temporal_series[:,(index - s):index]\n",
    "    return (previous_data, expected_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa5f37-1ad1-4f97-9b85-08c1e1a0348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_medium_time(temporal_series: np.array, index: int, m: int, mm: int) -> tuple:\n",
    "    \"\"\"\n",
    "        Generate medium time data in format of numpy array\n",
    "\n",
    "        Parameters:\n",
    "        - temporal_series: A numpy array that represents a temporal series\n",
    "        - index: The sample index\n",
    "        - m: The number of samples in a medium time data\n",
    "        - mm: interval between selected samples of the temporal series.\n",
    "             This value must be an integer representing the number of time series samples to be discarded\n",
    "\n",
    "        Returns:\n",
    "        A tuple that contains a numpy array of the medium time data and the corresponding expected data\n",
    "    \"\"\"\n",
    "    expected_vector = temporal_series[:,index]\n",
    "    m *= mm\n",
    "    previous_data = temporal_series[:,(index-m):index:mm]\n",
    "    return (previous_data, expected_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1bfc8-295d-49f4-ab63-17b4cb3835ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_long_time(temporal_series: np.array, index: int, l: int, ll: int) -> tuple:\n",
    "    \"\"\"\n",
    "        Generate long time data in format of numpy array\n",
    "\n",
    "        Parameters:\n",
    "        - temporal_series: A numpy array that represents a temporal series\n",
    "        - index: The sample index\n",
    "        - l: The number of samples in a long time data\n",
    "        - ll: interval between selected samples of the temporal series.\n",
    "             This value must be an integer representing the number of time series samples to be discarded\n",
    "\n",
    "        Returns:\n",
    "        A tuple that contains a numpy array of the long time data and the corresponding expected data\n",
    "    \"\"\"\n",
    "    expected_vector = temporal_series[:,index]\n",
    "    l *= ll\n",
    "    previous_data = temporal_series[:,(index - l):index:ll]\n",
    "    return (previous_data, expected_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1f276-8cd6-4677-82da-43d40f81fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenation(short_data: np.array, medium_data: np.array, long_data: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "        Concatenate short, medium and long term data\n",
    "    \"\"\"\n",
    "    return np.concatenate((short_data, medium_data, long_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497974f9-d978-4700-a302-c17186c4cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elements_medium_term(medium_timedelta: int, sample_timedelta: int) -> int:\n",
    "    \"\"\"\n",
    "        Calculate the number of elements in a medium term\n",
    "\n",
    "        Args:\n",
    "        - medium_timedelta: The timedelta between the initial and the final sample\n",
    "        of the medium time data\n",
    "        - sample_timedelta: The timedelta between consecutive samples of the medium time data\n",
    "    \"\"\"\n",
    "    current_time = dt.datetime.now()\n",
    "    td = dt.timedelta(hours=medium_timedelta)\n",
    "    initial_time = current_time - td\n",
    "    count = 0\n",
    "    while initial_time < current_time:\n",
    "        count += 1\n",
    "        time_change = dt.timedelta(minutes=sample_timedelta)\n",
    "        initial_time += time_change\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3a697-eee2-4702-ac4d-e09f56f79b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elements_long_term(long_timedelta: int, sample_timedelta: int) -> int:\n",
    "    \"\"\"\n",
    "        Calculate the number of elements in a long term\n",
    "\n",
    "        Args:\n",
    "        - long_timedelta: The timedelta between the initial and the final sample\n",
    "        of the long time data\n",
    "        - sample_timedelta: The timedelta between consecutive samples of the long time data\n",
    "    \"\"\"\n",
    "    current_time = dt.datetime.now()\n",
    "    td = dt.timedelta(hours=long_timedelta)\n",
    "    initial_time = current_time - td\n",
    "    count = 0\n",
    "    while initial_time < current_time:\n",
    "        count += 1\n",
    "        time_change = dt.timedelta(minutes=sample_timedelta)\n",
    "        initial_time += time_change\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c0051a-8f5a-46d5-a1e5-cb7e24c118c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Xt(temporal_series: np.array, index: int, sample_timedelta: int, medium_timedelta: int, long_timedelta: int, mm: int, ll: int) -> tuple:\n",
    "    \"\"\"\n",
    "        Create a tuple of X and y input of the GNN\n",
    "\n",
    "        Args:\n",
    "        - temporal_series: A numpy array that represents a temporal series\n",
    "        - index: The sample index\n",
    "        - sample_timedelta: The timedelta betwenn consecutive samples\n",
    "        - medium_timedelta: The timedelta between the initial and the final sample\n",
    "        of the medium time data\n",
    "        - long_timedelta: The timedelta between the initial and the final sample\n",
    "        of the long time data\n",
    "        - mm: interval between selected samples of the temporal series.\n",
    "             This value must be an integer representing the number of time series samples to be discarded\n",
    "        - ll: interval between selected samples of the temporal series.\n",
    "             This value must be an integer representing the number of time series samples to be discarded\n",
    "    \"\"\"\n",
    "    s = 2\n",
    "    m = elements_medium_term(medium_timedelta, sample_timedelta * mm)\n",
    "    l = elements_long_term(long_timedelta, sample_timedelta * ll)\n",
    "    short_data, expected_data_1 = data_short_time(temporal_series, index, s)\n",
    "    medium_data, expected_data_2 = data_medium_time(temporal_series, index, m, mm)\n",
    "    long_data, expected_data_3 = data_long_time(temporal_series, index, l, ll)\n",
    "    return (concatenation(short_data, medium_data, long_data), expected_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a3a33-cfa3-4f31-b9d2-2713c25c8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_index(long_timedelta: int, sample_timedelta: int) -> int:\n",
    "    \"\"\"\n",
    "        Determine the initial index to generate the data\n",
    "\n",
    "        Args:\n",
    "        - long_timedelta: The timedelta between the initial and the final sample\n",
    "        of the long time data\n",
    "        - sample_timedelta: The timedelta betwenn consecutive samples\n",
    "    \"\"\"\n",
    "    delta = dt.timedelta(hours=long_timedelta)\n",
    "    now = dt.datetime.now()\n",
    "    tomorrow = now + delta\n",
    "    count = 0\n",
    "    while now <= tomorrow:\n",
    "        count += 1\n",
    "        time_change = dt.timedelta(minutes=sample_timedelta)\n",
    "        now += time_change\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b8853-7119-4c36-986c-0901459add03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_Xt(matrix: np.array, sample_timedelta: int, medium_timedelta, long_timedelta, mm: int, ll: int) -> list:\n",
    "    \"\"\"\n",
    "        Create a list of input data\n",
    "    \"\"\"\n",
    "    start_index = initial_index(long_timedelta, sample_timedelta)\n",
    "    list_Xt = []\n",
    "    lines, columns = matrix.shape[0], matrix.shape[1]\n",
    "    for i in range(start_index, columns):\n",
    "        tuplee = create_Xt(matrix, i, sample_timedelta, medium_timedelta, long_timedelta, mm, ll)\n",
    "        list_Xt.append(tuplee)\n",
    "    return list_Xt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74406546-32ec-40fb-8b0f-cf993784034b",
   "metadata": {},
   "source": [
    "# Methods - Separation of Data into Training, Validation and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca9cb5-f808-4fe9-9219-ec3faaae0c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(list_Xt: list, prob_training: float, prob_validation: float) -> tuple:\n",
    "    \"\"\"\n",
    "      Split the input data in training, validation and testing sets.\n",
    "\n",
    "      Args:\n",
    "      - list_Xt: List of input data\n",
    "      - prob_training: Probability of a input being placed in training set\n",
    "      - prob_validation: Probability of a input being placed in validation set\n",
    "    \"\"\"\n",
    "    training = []\n",
    "    validation = []\n",
    "    test = []\n",
    "    for i in range(len(list_Xt)):\n",
    "        val = np.random.rand()\n",
    "        if val < prob_training:\n",
    "            training.append(list_Xt[i])\n",
    "        elif val < prob_validation:\n",
    "            validation.append(list_Xt[i])\n",
    "        else:\n",
    "            test.append(list_Xt[i])\n",
    "    return (training, validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c76fe-be30-4ac8-bb8d-09e006a7ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_split(list_Xt: list, frac_training: float, frac_validation: float) -> tuple:\n",
    "    \"\"\"\n",
    "      Split the input data in training, validation and testing sets.\n",
    "\n",
    "      Args:\n",
    "      - list_Xt: List of input data\n",
    "      - frac_training: Fraction of the input that will be for training\n",
    "      - frac_validation: Fraction of the input that will be for validation\n",
    "    \"\"\"\n",
    "    training = []\n",
    "    validation = []\n",
    "    test = []\n",
    "    index_training = m.floor(len(list_Xt) * frac_training)\n",
    "    index_validation = index_training + m.floor(len(list_Xt) * (frac_validation - frac_training))\n",
    "    for i in range(len(list_Xt)):\n",
    "        if i < index_training:\n",
    "            training.append(list_Xt[i])\n",
    "        elif i < index_validation:\n",
    "            validation.append(list_Xt[i])\n",
    "        else:\n",
    "            test.append(list_Xt[i])\n",
    "    return (training, validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b9645-8f93-48e5-b68b-f532350f60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x_and_y(list_of_tuples: list) -> tuple:\n",
    "    \"\"\"\n",
    "        Split a list of tuples into two lists\n",
    "    \"\"\"\n",
    "    sett_x = []\n",
    "    sett_y = []\n",
    "\n",
    "    for _ in range(10):\n",
    "      random.shuffle(list_of_tuples)\n",
    "\n",
    "    for tuplee in list_of_tuples:\n",
    "        x, y = tuplee\n",
    "        sett_x.append(x)\n",
    "        sett_y.append(y)\n",
    "    return (np.array(sett_x), np.array(sett_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb98a17-35b6-43d1-be6a-1c78344a8739",
   "metadata": {},
   "source": [
    "# GNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31616c-a950-4f08-a101-40340c785f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_C(Dataset):\n",
    "    \"\"\"\n",
    "        Create a dataset of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, adjacency_matrix: np.array, list_Xt: list, **kwargs):\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "        self.list_Xt = list_Xt\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        list_graphs = []\n",
    "        for tuplee in self.list_Xt:\n",
    "            Xt, yt = tuplee\n",
    "            list_graphs.append(Graph(x=Xt, a=self.adjacency_matrix, e=None, y=yt))\n",
    "\n",
    "        return list_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95a6b6-bf08-47cb-bbd6-a4ae1aa803c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnableMatrixMultiplicationLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Class for learnable matrix multiplation layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int):\n",
    "        super(LearnableMatrixMultiplicationLayer, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Creates the learnable tensor with the correct dimensions\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[input_shape[-1], self.channels], trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Multiply the input tensor by the learnable tensor\n",
    "        return tf.matmul(inputs, self.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d208f-188c-4a09-a78d-234d73b90470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedMatrixMultiplicationLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Class for fixed matrix multiplation layer\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super(FixedMatrixMultiplicationLayer, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Creates the learnable tensor with the correct dimensions\n",
    "        input_x, input_y = input_shape\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[input_x[-1], self.channels],initializer=initializers.Ones(),trainable=False)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Multiply the input tensor by the learnable tensor\n",
    "        input1, input2 = inputs\n",
    "        return tf.matmul(input1,self.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9037fa-d988-42f2-b5e7-8a1f3a098942",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedMatrixMultiplicationLayer2(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Class for fixed matrix multiplation layer\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super(FixedMatrixMultiplicationLayer2, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Creates the learnable tensor with the correct dimensions\n",
    "        input_x = input_shape\n",
    "        #self.kernel = self.add_weight(\"kernel\", shape=[input_x[-1], input_x[-2]],initializer=initializers.Ones(),trainable=False)\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[input_x[-1], self.channels],initializer=initializers.Ones(),trainable=False)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Multiply the input tensor by the learnable tensor\n",
    "        input1 = inputs\n",
    "        return tf.matmul(input1,self.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc6b39-52e2-46fb-918c-75e60ca48a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true: np.array, y_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error\n",
    "    Args:\n",
    "        y_true ([np.array]): test samples\n",
    "        y_pred ([np.array]): predicted samples\n",
    "    Returns:\n",
    "        [float]: root mean squared error\n",
    "    \"\"\"\n",
    "    y_pred2 = tf.squeeze(y_pred)\n",
    "    return K.sqrt(K.mean(K.square(y_pred2 - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1367c-2ef1-4ca9-bc9c-5ff2530b8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrmse(y_true: np.array, y_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Normalized Root Mean Squared Error\n",
    "    Args:\n",
    "        y_true ([np.array]): test samples\n",
    "        y_pred ([np.array]): predicted samples\n",
    "    Returns:\n",
    "        [float]: normalized root mean squared error\n",
    "    \"\"\"\n",
    "    y_pred2 = tf.squeeze(y_pred)\n",
    "    return K.sqrt(K.mean(K.square(y_pred2 - y_true), axis=-1)) / K.mean(K.abs(y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce135c-ef7d-4255-8626-43ee0fea6428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error\n",
    "    \"\"\"\n",
    "    y_true_tensor = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred_tensor = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "    return K.sqrt(K.mean(K.square(y_pred_tensor - y_true_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f1d0a4-b053-438d-bcf0-585abe0a249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrmse2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Normalized Root Mean Squared Error\n",
    "    \"\"\"\n",
    "    y_true_tensor = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred_tensor = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "    \n",
    "    return K.sqrt(K.mean(K.square(y_pred_tensor - y_true_tensor))) / K.mean(y_true_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1a72de-54f6-409b-a1c0-7bfb8171c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(y_true: np.array, y_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "        Method for square error loss\n",
    "    \"\"\"\n",
    "    error = tf.square(y_true - y_pred)\n",
    "    loss = tf.reduce_mean(error)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca2694-a5c3-4ee5-97d7-efa35a06a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(Model):\n",
    "    \"\"\"\n",
    "        Class for GNN model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, k_layers: int, relu_last=False):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.k_layers = k_layers\n",
    "        self.num_layers = len(channels)\n",
    "        self.relu_last = relu_last\n",
    "        self.init_layers()\n",
    "\n",
    "    def init_layers(self):\n",
    "        self.concatenate = Concatenate(axis=2)\n",
    "        self.add = Add()\n",
    "        self.relu = tf.keras.layers.Activation('relu')\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            setattr(self, f'cheb_stgi_l{i+1}', [])\n",
    "            kk = self.k_layers[i]\n",
    "            for k in range(1, kk + 1):\n",
    "                layer = ChebConv(self.channels[i], K=k, activation='relu', use_bias=True)\n",
    "                getattr(self, f'cheb_stgi_l{i+1}').append(layer)\n",
    "\n",
    "        self.dot_learnable_layers = [LearnableMatrixMultiplicationLayer(self.channels[i]) for i in range(self.num_layers)]\n",
    "        self.dot_fixed_layers = [FixedMatrixMultiplicationLayer(self.channels[i]) for i in range(self.num_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, y = inputs\n",
    "        out = None\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            cheb_stgi_layers = getattr(self, f'cheb_stgi_l{i+1}')\n",
    "            out_layers = [cheb(inputs) for cheb in cheb_stgi_layers]\n",
    "            concatenate = self.concatenate(out_layers)\n",
    "            mult_learnable = self.dot_learnable_layers[i](concatenate)\n",
    "            mult_fixed = self.dot_fixed_layers[i](inputs)\n",
    "            add = self.add([mult_learnable, mult_fixed])\n",
    "            if i < self.num_layers - 1 or self.relu_last:\n",
    "                relu = self.relu(add)\n",
    "                out = relu\n",
    "            else:\n",
    "                out = add\n",
    "            inputs = (out,y)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c7c56c-43ba-49bb-8811-6e71040e201a",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3cd15-2de9-4713-b8a7-907803737e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"\" # fill out\n",
    "file_temporal_series = directory + \"normalized.npy\"\n",
    "file_transition_matrix = directory + \"Transition Matrix/transition_matrix.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f09e14-b660-433a-b690-c83415a5dee7",
   "metadata": {},
   "source": [
    "**Load Temporal Series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97729e7c-55c1-463f-98fe-d75d66aa2624",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_series = np.load(file_temporal_series)\n",
    "temporal_series = temporal_series.astype(float)\n",
    "lines, columns = temporal_series.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de33e81-67c6-4a20-9d60-c2daa3e41feb",
   "metadata": {},
   "source": [
    "**Load Transition Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7bbfe3-3767-4773-9bef-fac2ac850914",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = load_matrix(file_transition_matrix)\n",
    "matrix_sparse = sp.sparse.csr_matrix(transition_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d968ac-9ed8-4317-bdbf-1ac3c6481cb3",
   "metadata": {},
   "source": [
    "# Input Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd82538-bbbb-46a1-8053-72eefb13f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_interval = 5\n",
    "short_samples = 2\n",
    "medium_time = 24\n",
    "medium_samples = 6\n",
    "long_time = 168\n",
    "long_samples = 12\n",
    "sensors = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d8071-6db9-4c06-8d33-a71d3658aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_Xt = create_list_Xt(temporal_series, samples_interval, medium_time, long_time, medium_samples, long_samples)\n",
    "print(len(list_Xt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2d7ba-abbb-4ba7-a387-c2cbaf97dcf3",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cfe1bc-f80f-4c17-aed7-d282741c69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_array = [0.4,0.5]\n",
    "final_relu = False\n",
    "conv_array = [1]\n",
    "k_array = [4]\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca91ad1-e007-4c70-8d1e-582fabcdb8c2",
   "metadata": {},
   "source": [
    "# Data Separation into Training, Validation and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d731e38-6374-4b25-a4da-15458945b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_training(list_Xt, split_array, matrix_sparse):\n",
    "    training, validation, test = sequential_split(list_Xt,split_array[0],split_array[1])\n",
    "    return Dataset_C(matrix_sparse, training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5643ff65-49de-495f-b4e6-f36f93af9bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_validation(list_Xt,split_array,matrix_sparse):\n",
    "    training, validation, test = sequential_split(list_Xt,split_array[0],split_array[1])\n",
    "    return Dataset_C(matrix_sparse, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40461a-8f71-4912-ad3f-38e9d370c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_test(list_Xt,split_array,matrix_sparse):\n",
    "    training, validation, test = sequential_split(list_Xt,split_array[0],split_array[1])\n",
    "    return Dataset_C(matrix_sparse, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a81c0-dfd5-454d-bfcc-7233f76e4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_split_training(list_Xt,split_array,matrix_sparse)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113fc880-bbc4-4f66-b8fb-9f5e9750df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = data_split_validation(list_Xt,split_array,matrix_sparse)\n",
    "print(dataset_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e22fdb-550b-47c6-bae4-05b01ba7e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = data_split_test(list_Xt,split_array,matrix_sparse)\n",
    "print(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace5987-2f25-4fc6-a21a-9043b213e879",
   "metadata": {},
   "source": [
    "# GNN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b895ae7-25da-4927-a7df-fd756607c02c",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fe87a-092d-42dc-88ac-b35b6723e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_training = 1\n",
    "epochs_training = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685eb3a0-142e-487f-aa3c-847f0d449072",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, min_lr=0.00001)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36eec1e-2365-436d-8db0-d74b5e5098f5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823042cb-1d94-4c97-9ed7-e7cb35acc552",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(conv_array,k_array,final_relu)\n",
    "model.compile(optimizer=optimizer,loss=squared_error,metrics=[rmse,nrmse,\"mae\",\"mape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafc59e-54af-429b-809d-26cd8ea6584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = BatchLoader(dataset, batch_size=batch_size_training,shuffle=True)\n",
    "loader_validation = BatchLoader(dataset_validation, batch_size=batch_size_training)\n",
    "loader_test = BatchLoader(dataset_test, batch_size=batch_size_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048ef96-d6de-4657-ac84-778f56eed41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "metrics_fit = model.fit(loader.load(), use_multiprocessing=True, workers=-1, verbose=0, steps_per_epoch=loader.steps_per_epoch, epochs=epochs_training, validation_data=loader_validation.load(), validation_steps=loader_validation.steps_per_epoch, callbacks=[reduce_lr, tensorboard_callback])\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0949192-0d31-4f3f-86dc-049b769984cd",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434717c-2b69-413a-bf29-9fef1e89bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "metrics_validation = model.evaluate(loader_validation.load(), steps=loader_validation.steps_per_epoch)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c27158-12b5-4615-beb7-ddff66ec5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "metrics_testing = model.evaluate(loader_test.load(), steps=loader_test.steps_per_epoch)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b95ad7-23d4-4302-aa73-0dffe5d17108",
   "metadata": {},
   "source": [
    "# Data Separation for Linear Regression and FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c00f176-35d6-40ee-b51f-55d078cc18fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_training(list_Xt,split_array):\n",
    "    training, validation, test = sequential_split(list_Xt,split_array[0],split_array[1])\n",
    "    x_training, y_training = split_x_and_y(training)\n",
    "    return (x_training, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b117bc42-88c9-4291-8feb-83efdf139874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_validation(list_Xt,split_array):\n",
    "    training, validation, test = sequential_split(list_Xt,split_array[0],split_array[1])\n",
    "    x_validation, y_validation = split_x_and_y(validation)\n",
    "    return (x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfcfc09-ea9b-4cf1-b47e-60b01bd6d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_test(list_Xt,split_array):\n",
    "    training, validation, test = sequential_split(list_Xt,split_array[0],split_array[1])\n",
    "    x_test, y_test = split_x_and_y(test)\n",
    "    return (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4db8267-0bf0-496a-9629-c9b3a9f4ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_training(matrix_flow, samples_interval, medium_time, long_time, medium_samples, long_samples, split_array):\n",
    "    list_Xt = create_list_Xt(matrix_flow, samples_interval, medium_time, long_time, medium_samples, long_samples)\n",
    "    return  data_split_training(list_Xt,split_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d33ac-24a6-4e90-a9da-7e886ed9d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_validation(matrix_flow, samples_interval, medium_time, long_time, medium_samples, long_samples, split_array):\n",
    "    list_Xt = create_list_Xt(matrix_flow, samples_interval, medium_time, long_time, medium_samples, long_samples)\n",
    "    return  data_split_validation(list_Xt,split_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c7a05-d2aa-4420-8e4a-7c8023ea8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_test(matrix_flow, samples_interval, medium_time, long_time, medium_samples, long_samples, split_array):\n",
    "    list_Xt = create_list_Xt(matrix_flow, samples_interval, medium_time, long_time, medium_samples, long_samples)\n",
    "    return  data_split_test(list_Xt,split_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76ad8c-b018-4faf-bafb-21d18e666c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training, y_training = data_split_training(list_Xt,split_array)\n",
    "x_validation, y_validation = data_split_validation(list_Xt,split_array)\n",
    "x_test, y_test = data_split_test(list_Xt,split_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ea002-52a2-4168-b6e7-cd7da795b56f",
   "metadata": {},
   "source": [
    "# Linear Regression Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8848af-1266-4f68-a3ff-fe86edd72de2",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2437e9-a353-4920-8f79-4751a0983546",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_training = 1\n",
    "epochs_training = 30\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf9799b-f992-4def-b4c8-a19c5a0e0cee",
   "metadata": {},
   "source": [
    "## Linear Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd0695e-1c87-4aa2-aca4-4cd8170ed07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin, col = x_training[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d640b-9497-4799-8b30-6e71da94728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(lin, col)),  \n",
    "    tf.keras.layers.Dense(1)                \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158d457-51dd-4974-902d-69fa0cbd06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=squared_error,metrics=[rmse,nrmse,\"mae\",\"mape\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed049e1-60b2-4473-9346-edaec9aa7bb5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b59083-7d96-4346-b176-d9652c05dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "metrics_fit = model.fit(x_training, y_training, epochs=epochs_training, batch_size=batch_size_training, validation_data=(x_validation, y_validation), verbose=0, callbacks=[reduce_lr,tensorboard_callback])\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd53be82-2ca9-4c77-9f5f-597d646cfa7b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f848ed-6ac8-4e01-a85a-0c24a64c0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "metrics_validation = model.evaluate(x_validation,y_validation)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84fa4e-ce7b-4400-a96e-a52bf36f2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "metrics_testing = model.evaluate(x_test,y_test)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c660a3-bf87-410e-a225-e4a436d7e1b1",
   "metadata": {},
   "source": [
    "# FCN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09fb70c-8ecd-4fe0-8fe6-0ab6c30edb4c",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f002b8e-b4c0-4974-82ba-1f7aa6fb9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_training = 1\n",
    "epochs_training = 30\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d28ef-319e-43de-8ccd-12d4cdbc7677",
   "metadata": {},
   "source": [
    "## FCN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cdda8-de0e-4ed5-99ae-cedeb77f07ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin, col = x_training[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516875a6-8ffc-4346-82ee-dd7acaf3fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(lin,col)),              \n",
    "    tf.keras.layers.Dense(128, activation='relu'),  \n",
    "    tf.keras.layers.Dropout(0.2),                  \n",
    "    tf.keras.layers.Dense(1)                     \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c1814-0642-40e6-be6c-5af9fa883197",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse',metrics=[rmse,nrmse,\"mae\",\"mape\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709559c1-b327-402f-b7a1-1db6c7149c1a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd43fef4-51cb-4538-adb3-3109e10adee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "metrics_fit = model.fit(x_training, y_training, epochs=epochs_training, batch_size=batch_size_training, validation_data=(x_validation, y_validation), verbose=0, callbacks=[reduce_lr,tensorboard_callback])\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e3dcb-cac3-407f-8307-d480b716e662",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0891b32-afeb-490a-9243-9d38552ed68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "metrics_validation = model.evaluate(x_validation,y_validation)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc8101-9c79-41f9-bb82-ac5a995c01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "metrics_testing = model.evaluate(x_test,y_test)\n",
    "end_time = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
