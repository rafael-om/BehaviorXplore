{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb571cb-c3ad-4fbe-a709-4aeea7b86b73",
   "metadata": {},
   "source": [
    "### Author: Rafael de Oliveira MagalhÃ£es\n",
    "\n",
    "# PEMSd3 Dataset - Data Cleaning\n",
    "\n",
    "# Download Data\n",
    "\n",
    "All data can be downloaded on the website https://pems.dot.ca.gov/.\n",
    "\n",
    "First, it is necessary to create an account to access the data.\n",
    "\n",
    "After logging in, go to 'Data Clearinghouse'. Then access:\n",
    "\n",
    "- 'Station 5-Minute' -> 'District 3'. On this page, you will find data captured by sensors from 2001 to the present moment. Each day captured by the sensors is recorded in a single .txt file. To simplify the download, it is recommended to use an extension to download multiple files automatically.\n",
    "- 'Station Metadata' -> 'District 3'. To download metadata files for the sensors, which are used to generate a map of monitored roadways.\n",
    "\n",
    "Additionally, a list with a subset of the sensors is in the PEMSd3.csv file\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a6456-b096-4057-8237-82210733ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, SeparableConv2D\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.metrics import Metric\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Add, Concatenate, Input, GlobalAveragePooling2D, Layer\n",
    "from keras import models, initializers\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "from spektral.datasets import TUDataset\n",
    "from spektral.layers import GCNConv, GlobalSumPool, ChebConv\n",
    "from spektral.data import SingleLoader, BatchLoader\n",
    "from spektral.data import Graph\n",
    "from spektral.data import Dataset\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, acorr_ljungbox\n",
    "from scipy.stats import f_oneway, f, kstest, norm, ks_2samp, kendalltau\n",
    "from scipy.interpolate import interp2d, RegularGridInterpolator, RectBivariateSpline, griddata\n",
    "\n",
    "# Helper libraries\n",
    "\n",
    "from bokeh.io import show\n",
    "from bokeh.plotting import gmap\n",
    "from bokeh.models import GMapOptions\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "import csv\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import folium\n",
    "import pyproj\n",
    "import math as m\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "import datetime as dt\n",
    "import re\n",
    "import time\n",
    "import gmaps as gm\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely import wkt\n",
    "from numba import jit, cuda\n",
    "from sodapy import Socrata\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings as w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e3b130-2747-44fa-b881-659ebe3c30db",
   "metadata": {},
   "source": [
    "# Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ab7cd-352a-41dc-aa9b-4902b8e89d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict(array: list) -> dict:\n",
    "    \"\"\"\n",
    "        Generate a dict from a array\n",
    "    \"\"\"\n",
    "    dictionary = {value: index for index, value in enumerate(array)}\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad9454-f866-4695-b875-6694fc18166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_sort(dictt):\n",
    "    return dict(sorted(dictt.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f1de40-58ed-4aea-98e6-40757c1777ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(element, array: list):\n",
    "    lo = 0\n",
    "    hi = len(array) - 1\n",
    "    while lo <= hi:\n",
    "        mid = lo + (hi - lo)//2\n",
    "        temp = array[mid]\n",
    "        if element > temp:\n",
    "            lo = mid + 1\n",
    "        elif element < temp:\n",
    "            hi = mid - 1\n",
    "        else:\n",
    "            return mid\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff300e02-5967-48cc-9a72-cdc7f28f8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_datetime(initial_date: dt.datetime, length: int) -> list:\n",
    "    \"\"\"\n",
    "        Create a list of datetime object that represents a time series\n",
    "\n",
    "        Parameters:\n",
    "        - initial_date: Initial date of the list\n",
    "        - length: Expected length of the generated list\n",
    "    \"\"\"\n",
    "    date_start = initial_date\n",
    "    array_dates = []\n",
    "    i = 0\n",
    "    while i < length:\n",
    "        array_dates.append(date_start)\n",
    "        time_change = dt.timedelta(minutes=5)\n",
    "        date_start += time_change\n",
    "        i += 1\n",
    "    return array_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f6cc4-fe4f-44d0-aaaa-f2dcaecda987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_datetime_timedelta(initial_date: dt.datetime, final_date: dt.datetime, timedelta: int) -> list:\n",
    "    \"\"\"\n",
    "        Create a list of datetime objects by increasing time by timedelta\n",
    "    \"\"\"\n",
    "    date_start = initial_date\n",
    "    array_dates = []\n",
    "    while date_start <= final_date:\n",
    "        array_dates.append(date_start)\n",
    "        time_change = dt.timedelta(minutes=timedelta)\n",
    "        date_start += time_change\n",
    "    return array_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999584b9-e0cc-43e7-9936-672a57cef4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix: np.array) -> np.array:\n",
    "    normalized_arr = (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "    return normalized_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de74405-b97c-42fc-b669-a87c80a0dd52",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74819f35-6a1f-497d-b1b2-08bfceaff76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(archive: str, directory_initial: str, directory_destiny: str, sensors, specific_sensor=None) -> None:\n",
    "    \"\"\"\n",
    "        Cleans texts files by deleting unnecessary columns\n",
    "    \"\"\"\n",
    "    new_name = directory_destiny + \"/\" + archive[:-3] + \"csv\"\n",
    "    og_name = directory_initial + \"/\" + archive\n",
    "    with open(new_name, 'w+') as new_arc:\n",
    "        with open(og_name, \"r\") as arc:\n",
    "            first_line = 'Timestamp,Station,Flow,Speed\\n'\n",
    "            new_arc.write(first_line)\n",
    "            for line in arc:\n",
    "                content = line.split(\",\")\n",
    "                try:\n",
    "                    sensor = int(content[1])\n",
    "                except:\n",
    "                    continue\n",
    "                if specific_sensor is not None:\n",
    "                    if sensor == specific_sensor:\n",
    "                        new_line = content[0] + \",\" + content[1] + \",\" + content[9] + \",\" + content[11] + \"\\n\"\n",
    "                        new_arc.write(new_line)\n",
    "                else:\n",
    "                    if sensor in sensors:\n",
    "                        new_line = content[0] + \",\" + content[1] + \",\" + content[9] + \",\" + content[11] + \"\\n\"\n",
    "                        new_arc.write(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ced668-f100-4f8b-9ddd-b39cf9a0d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def through_directory(directory_initial: str, directory_csv_files: str, sensors, specific_sensor=None) -> None:\n",
    "    \"\"\"\n",
    "        Cleans text files from a directory and creates csv files\n",
    "    \"\"\"\n",
    "    cont = 0\n",
    "    for name_archive in os.listdir(directory_initial):\n",
    "        print(cont)\n",
    "        if name_archive.endswith('.txt'):\n",
    "            clean_txt(name_archive, directory_initial, directory_csv_files, sensors, specific_sensor)\n",
    "            cont += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8191f17-3945-4442-9fdf-145e1f1859dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sensors = generate_dict_sensors(\"Adjacency_list.csv\")\n",
    "list_sensors = set(dict_sensors.keys())\n",
    "directory_initial = \"\" # fill out\n",
    "directory_csv_files = \"\" # fill out\n",
    "through_directory(directory_initial, directory_csv_files, list_sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f57112a-c663-47f6-aa51-acd106e13d42",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1c581-7c43-40e1-8958-f515fd9d82cc",
   "metadata": {},
   "source": [
    "## Methods - Select Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34785cdd-3998-4b5e-99c7-d018ca91de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict_sensors(archive: str) -> dict:\n",
    "    \"\"\"\n",
    "      Read a csv file that contains the sensors and create a sensors dictionary\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(archive).dropna()\n",
    "    unique_stations = pd.unique(dataset['from'])\n",
    "    return generate_dict(unique_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc36da66-f59b-4144-8138-7ba63a9afe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_sensors(dict_sensors: dict, exclude_lines: list) -> dict:\n",
    "    \"\"\"\n",
    "        Exclude invalid sensors from the sensors dictionary\n",
    "    \"\"\"\n",
    "    new_sensors = []\n",
    "    sensors = list(dict_sensors.keys())\n",
    "    for i in range(len(sensors)):\n",
    "        if i not in exclude_lines:\n",
    "            new_sensors.append(sensors[i])\n",
    "    return generate_dict(new_sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c3a8d-0f4f-4a88-832b-cf1a2c7701a0",
   "metadata": {},
   "source": [
    "## Methods - Generate Temporal Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a824ef0-fff3-43fe-8ce0-1f3ea4e8cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_flow_matrix(dict_dates: dict, dict_sensors: dict, directory: str) -> np.array:\n",
    "  \"\"\"\n",
    "      Insert flow data in temporal series\n",
    "  \"\"\"\n",
    "  matrix = np.full((len(dict_sensors), len(dict_dates)), np.nan)\n",
    "  for name_archive in os.listdir(directory):\n",
    "    if name_archive.endswith('.csv'):\n",
    "        print(name_archive)\n",
    "        archive = directory + \"/\" + name_archive\n",
    "        dataset = pd.read_csv(archive)\n",
    "        for i in range(len(dataset)):\n",
    "          line = dataset.iloc[i]\n",
    "          time = line['Timestamp']\n",
    "          sensor = line['Station']\n",
    "          try:\n",
    "            ii = dict_sensors[sensor]\n",
    "          except:\n",
    "            continue\n",
    "          format = \"%m/%d/%Y %H:%M:%S\"\n",
    "          date = dt.datetime.strptime(time, format)\n",
    "          j = dict_dates[date]\n",
    "          matrix[ii][j] = line['Flow']\n",
    "  return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9490880-9428-477d-bf69-a9baba8e910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_flow_list(dict_dates: dict, directory: str) -> np.array:\n",
    "  \"\"\"\n",
    "      Insert flow data in temporal series\n",
    "  \"\"\"\n",
    "  matrix = np.full((len(dict_dates)), np.nan)\n",
    "  for name_archive in os.listdir(directory):\n",
    "    if name_archive.endswith('.csv'):\n",
    "        print(name_archive)\n",
    "        archive = directory + \"/\" + name_archive\n",
    "        dataset = pd.read_csv(archive)\n",
    "        for i in range(len(dataset)):\n",
    "          line = dataset.iloc[i]\n",
    "          time = line['Timestamp']\n",
    "          format = \"%m/%d/%Y %H:%M:%S\"\n",
    "          date = dt.datetime.strptime(time, format)\n",
    "          j = dict_dates[date]\n",
    "          matrix[j] = line['Flow']\n",
    "  return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548568d0-eb6a-4a05-a9b5-df2f954df071",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_csv_files = \"\" # fill out\n",
    "initial_date = dt.datetime(2023, 1, 1, 0, 0, 0)\n",
    "final_date = dt.datetime(2023, 8, 31, 23, 55, 0)\n",
    "list_datetime = list_datetime_timedelta(initial_date, final_date, 5)\n",
    "dict_dates = generate_dict(list_datetime)\n",
    "matrix_flow = insert_flow_matrix(dict_dates, dict_sensors, directory_csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06afac67-29c0-477e-bf60-d5cfdea5a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"matrix_pemsd3.npy\", matrix_flow) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d0577-1291-40ef-99ac-1a113d82b2a7",
   "metadata": {},
   "source": [
    "### Methods - Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b654c1-5a10-4ebf-86d0-cf42a4535b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliate_nan_values(matrix: np.array) -> tuple:\n",
    "    \"\"\"\n",
    "        Evaluate the amount of NaN values, of non NaN values, the percentage of NaN values and the list of NaN lines\n",
    "    \"\"\"\n",
    "    lin, col = matrix.shape\n",
    "    nan_lines = pd.DataFrame(columns=['NaN Values', 'Percentage'])\n",
    "    count_nan = 0\n",
    "    total = lin * col\n",
    "    for i in range(lin):\n",
    "        line = matrix[i]\n",
    "        known_indexes = np.arange(len(line))[~np.isnan(line)]\n",
    "        if len(known_indexes) == 0:\n",
    "            nan_lines.loc[i] = [m.inf, m.inf]\n",
    "            total -= col\n",
    "            continue\n",
    "        unknown_indexes = np.arange(len(line))[np.isnan(line)]\n",
    "        count_nan += len(unknown_indexes)\n",
    "        nan_lines.loc[i] = [len(unknown_indexes), len(unknown_indexes)/col * 100]\n",
    "    return (count_nan, total, count_nan/total * 100, nan_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7121be-5848-452c-96c8-71f26a401911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliate_nan_values_list(matrix: np.array) -> tuple:\n",
    "    \"\"\"\n",
    "        Evaluate the amount of NaN values, of non NaN values, the percentage of NaN values and the list of NaN lines\n",
    "    \"\"\"\n",
    "    length = len(matrix)\n",
    "    nan_lines = pd.DataFrame(columns=['NaN Values', 'Percentage'])\n",
    "    known_indexes = np.arange(length)[~np.isnan(matrix)]\n",
    "    if len(known_indexes) == 0:\n",
    "        nan_lines.loc[0] = [m.inf, m.inf]\n",
    "    else:\n",
    "        unknown_indexes = np.arange(length)[np.isnan(matrix)]\n",
    "        nan_lines.loc[0] = [len(unknown_indexes), len(unknown_indexes)/length * 100]\n",
    "    return nan_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b966f49a-517d-4102-a580-bb2b5e07e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_list(matrix: np.array) -> tuple:\n",
    "    \"\"\"\n",
    "        Interpolate a numpy array to fill the NaN values and exclude the NaN lines\n",
    "    \"\"\"\n",
    "    length = len(matrix)\n",
    "    known_indexes = np.arange(length)[~np.isnan(matrix)] \n",
    "    # Find the index of null values (NaN)\n",
    "    unknown_indexes = np.arange(length)[np.isnan(matrix)]\n",
    "    # Use the interp function to calculate estimated values for NaN\n",
    "    estimated_values = np.interp(unknown_indexes, known_indexes, matrix[~np.isnan(matrix)])\n",
    "    # Replace NaN values with estimated values\n",
    "    matrix[unknown_indexes] = estimated_values\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8cb54-2d4e-4969-b82a-6382067931cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_matrix(matrix: np.array) -> tuple:\n",
    "    \"\"\"\n",
    "        Interpolate a numpy array to fill the NaN values and exclude the NaN lines\n",
    "    \"\"\"\n",
    "    lin, col = matrix.shape\n",
    "    exclude_lines = []\n",
    "    for i in range(lin):\n",
    "        line = matrix[i]\n",
    "        known_indexes = np.arange(len(line))[~np.isnan(line)]\n",
    "        if len(known_indexes) == 0:\n",
    "            exclude_lines.append(i)\n",
    "            continue\n",
    "        # Find the index of null values (NaN)\n",
    "        unknown_indexes = np.arange(len(line))[np.isnan(line)]\n",
    "        if len(unknown_indexes) == 0:\n",
    "            continue\n",
    "        # Use the interp function to calculate estimated values for NaN\n",
    "        estimated_values = np.interp(unknown_indexes, known_indexes, line[~np.isnan(line)])\n",
    "        # Replace NaN values with estimated values\n",
    "        line[unknown_indexes] = estimated_values\n",
    "        matrix[i] = line\n",
    "    return (matrix, exclude_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f20c0d3-fe57-4351-9a33-e36a3613a5b8",
   "metadata": {},
   "source": [
    "### Methods - Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0fbdc0-725b-461b-a1ed-5380d691986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transition_matrix(dict_sensors,exclude_lines):\n",
    "  matrix = np.zeros((len(dict_sensors),len(dict_sensors)))\n",
    "  graph_csv = pd.read_csv(\"Adjacency_list.csv\")\n",
    "  graph_csv = graph_csv.dropna()\n",
    "  for i in range(len(graph_csv)):\n",
    "    line = graph_csv.iloc[i]\n",
    "    #if line['from'] in exclude_lines or line['to'] in exclude_lines:\n",
    "        #continue\n",
    "    try:\n",
    "        ii = dict_sensors[int(line['from'])]\n",
    "        j = dict_sensors[int(line['to'])]\n",
    "    except:\n",
    "        continue\n",
    "    matrix[ii][j] = line['distance']\n",
    "  return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa8d5a-4830-4633-8c1c-3d234d8732e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_avg(dict_sensors: dict, directory_initial: str) -> tuple:\n",
    "  list_max = np.zeros(len(dict_sensors))\n",
    "  list_count = np.zeros(len(dict_sensors))\n",
    "\n",
    "  for i in range(len(list_max)):\n",
    "    list_max[i] = -m.inf\n",
    "\n",
    "  for name_archive in os.listdir(directory_initial):\n",
    "    if name_archive.endswith('.csv'):\n",
    "      print(name_archive)\n",
    "      archive = directory_initial + \"/\" + name_archive\n",
    "      dataset = pd.read_csv(archive)\n",
    "      dataset = dataset.dropna(subset=['Speed'])\n",
    "      for i in range(len(dataset)):\n",
    "        line = dataset.iloc[i]\n",
    "        sensor = line['Station']\n",
    "        speed = line['Speed']\n",
    "        try:\n",
    "            index = dict_sensors[sensor]\n",
    "        except:\n",
    "            continue\n",
    "        list_count[index] += 1\n",
    "        if speed > list_max[index]:\n",
    "          list_max[index] = speed\n",
    "  return (list_max, list_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60875432-b1f8-44cf-ab11-29e09fd7a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_speed(dict_sensors: dict, list_count: list, directory_initial: str) -> np.array:\n",
    "  \"\"\"\n",
    "      Generate a numpy array of average speed for each node\n",
    "  \"\"\"\n",
    "  list_avg = np.zeros(len(dict_sensors), dtype=float)\n",
    "  for name_archive in os.listdir(directory_initial):\n",
    "    if name_archive.endswith('.csv'):\n",
    "      print(name_archive)\n",
    "      archive = directory_initial + \"/\" + name_archive\n",
    "      dataset = pd.read_csv(archive)\n",
    "      dataset = dataset.dropna(subset=['Speed'])\n",
    "      for i in range(len(dataset)):\n",
    "        line = dataset.iloc[i]\n",
    "        sensor = line['Station']\n",
    "        speed = line['Speed']\n",
    "        try:\n",
    "            index = dict_sensors[sensor]\n",
    "        except:\n",
    "            continue\n",
    "        list_avg[index] += speed / list_count[index]\n",
    "  return list_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f718d7e-3135-4688-a87d-81116b124b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_values(listt: np.array, value) -> np.array:\n",
    "    \"\"\"\n",
    "      Replace values equals 'value' by the mean of the other values\n",
    "    \"\"\"\n",
    "    sum_values = 0.0\n",
    "    count = 0\n",
    "    list_index = []\n",
    "    for i in range(len(listt)):\n",
    "        elem = listt[i]\n",
    "        if elem != value:\n",
    "            sum_values += elem\n",
    "            count += 1\n",
    "        else:\n",
    "            list_index.append(i)\n",
    "    for elem in list_index:\n",
    "        listt[elem] = sum_values/count\n",
    "    return listt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f2b20-f41f-4061-9ecb-4a3dc7546bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_matrix_definitive(matrix_og: np.array, exclude_lines: list) -> np.array:\n",
    "    \"\"\"\n",
    "        Exclude sensors (lines and columns) from the transition/adjacency matrix\n",
    "    \"\"\"\n",
    "    lin, col = matrix_og.shape\n",
    "    for i in range(lin):\n",
    "        for j in range(len(exclude_lines)):\n",
    "            jj = exclude_lines[j]\n",
    "            elem = matrix_og[i][jj]\n",
    "            if elem != 0.0:\n",
    "                for k in range(col):\n",
    "                    val = matrix_og[jj][k]\n",
    "                    if val != 0.0 and matrix_og[i][k] == 0.0:\n",
    "                        matrix_og[i][k] = val\n",
    "    matrix_og = np.delete(matrix_og, exclude_lines, axis=0)\n",
    "    matrix_og = np.delete(matrix_og, exclude_lines, axis=1)\n",
    "    return matrix_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ff2dd-47c1-479d-accf-1e96378e2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def definitive_transition_matrix(matrix: np.array, list_max: np.array, list_avg: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "        Fill the transition matrix\n",
    "\n",
    "        Args:\n",
    "        - matrix: The transition matrix\n",
    "        - list_max: The list of max speed for each node\n",
    "        - list_avg: The list of average speed for each node\n",
    "    \"\"\"\n",
    "    lin, col = matrix.shape\n",
    "    print(lin, col)\n",
    "    print(len(list_max), len(list_avg))\n",
    "    for i in range(lin):\n",
    "        count = 0\n",
    "        for j in range(col):\n",
    "            if matrix[i][j] != 0.0 and i != j:\n",
    "                count += 1\n",
    "\n",
    "        if count == 0:\n",
    "            matrix[i][i] = 1\n",
    "            print(i)\n",
    "            continue\n",
    "\n",
    "        matrix[i][i] = (list_max[i] - list_avg[i])/list_avg[i]\n",
    "\n",
    "        for j in range(col):\n",
    "            if matrix[i][j] != 0.0 and i != j:\n",
    "                matrix[i][j] = (1 - matrix[i][i])/count\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23db9d0-4810-4438-b97e-4d01133dd4bb",
   "metadata": {},
   "source": [
    "### Generate Transition Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8b072-e5d9-418f-880d-b77b6b74de28",
   "metadata": {},
   "source": [
    "**Load Transition Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52854df-8856-48bb-a565-b1e850ecda4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensors without data\n",
    "exclude_lines = [4, 78, 85, 198, 260, 316, 330, 331, 332, 333, 334, 338, 339]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed59869-f7e5-4833-9bd1-c3b92c87c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sensors = generate_dict_sensors(\"Adjacency_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593f5302-e6d5-4cb7-ae76-1aed9d0f8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = generate_transition_matrix(dict_sensors, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709899af-6b05-4568-8e72-d628e8e3d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = transition_matrix_definitive(transition_matrix, exclude_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee750e-8e47-4126-bc2e-dd6aa3a30611",
   "metadata": {},
   "source": [
    "**Load Speed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad8c3b-31bf-4949-a256-91d4820854ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf = pd.read_csv('max_speed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e040f3f-92ed-4cbd-ae29-aa4046c96510",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg = pd.read_csv('avg_speed.csv')\n",
    "list_avg = df_avg['Avg Speed'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8138931-d0d4-43fe-9778-520c29ed05f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_max = dataf['Max Speed'].values\n",
    "list_count = dataf['Count'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf1caf-5c8e-4f0e-8f14-a8d573470e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_max = interpolate_values(list_max, -m.inf)\n",
    "list_avg = interpolate_values(list_avg, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675aa52-da95-4a75-896f-ad3896e9f223",
   "metadata": {},
   "source": [
    "**Definitive Transition Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86994234-7d5d-4bba-b2ba-4ba9ed0f31c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = definitive_transition_matrix(transition_matrix, list_max, list_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f49010-8a12-4c1e-80a3-9ceca532dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sparse = sp.sparse.csr_matrix(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5954342d-1697-461b-96b2-c7b29b6bb1b8",
   "metadata": {},
   "source": [
    "# Methods - Temporal Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9058e-3cfb-41cd-b38b-aaea903b6918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_data(matrix: np.array, interval: int) -> np.array:\n",
    "    \"\"\"\n",
    "        Removes columns from a temporal series\n",
    "    \"\"\"\n",
    "    lin, col = matrix.shape\n",
    "    list_index = np.zeros(col) == 1\n",
    "    i = 0\n",
    "    while i < col:\n",
    "        list_index[i] = True\n",
    "        i += interval\n",
    "    return matrix[:,list_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1bea5-8fcc-4bdb-9f9f-f8279dcca305",
   "metadata": {},
   "source": [
    "# Methods - Remove Data for Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53801f5-d4f5-45ca-a81a-0f408dc8cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nan_columns(matrix: np.array, interval: int) -> np.array:\n",
    "    \"\"\"\n",
    "        Creates NaN columns into a temporal series\n",
    "    \"\"\"\n",
    "    lin, col = matrix.shape\n",
    "    nan_column = np.full((lin), np.nan)\n",
    "    i = 0\n",
    "    while i < col:\n",
    "        if (i % (interval + 1) != 0):\n",
    "            matrix[:, i] = nan_column\n",
    "        i += 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddab6104-368b-4660-8c4f-313742026177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_random_data(matrix: np.array, probability: float, length) -> np.array:\n",
    "    \"\"\"\n",
    "        Replaces values to NaN\n",
    "    \"\"\"\n",
    "    lin, col = matrix.shape\n",
    "    for i in range(lin):\n",
    "        for j in range(length):\n",
    "            prob = random.random()\n",
    "            if prob < probability:\n",
    "                matrix[i][j] = np.nan\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9419c-d914-47cb-b270-da0c0b81688f",
   "metadata": {},
   "source": [
    "# Methods - Sensor Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e2b5a8-6b9d-4697-8423-77d94898e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_index(length: int, probability: float) -> np.array:\n",
    "    \"\"\"\n",
    "        Creates a boolean array of sensors that will be excluded\n",
    "    \"\"\"\n",
    "    array = np.ones((length), dtype=int)\n",
    "    for i in range(length):\n",
    "        prob = random.random()\n",
    "        if prob < probability:\n",
    "            array[i] = 0\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6952886-f277-491c-85ec-16f9ce70628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sensors(matrix: np.array, list_index: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "        Remove sensors from a adjacency matrix\n",
    "    \"\"\"\n",
    "    lin, col = matrix.shape\n",
    "\n",
    "    for i in range(len(list_index)):\n",
    "        if list_index[i] == 0:\n",
    "            adj_i = []\n",
    "            for j in range(col):\n",
    "                if matrix[i][j] != 0.0:\n",
    "                    adj_i.append(j)\n",
    "\n",
    "            for j in range(lin):\n",
    "                if matrix[j][i] != 0.0:\n",
    "                    for k in adj_i:\n",
    "                        matrix[j][k] = 1\n",
    "\n",
    "    boolean_array = list_index == 1\n",
    "    matrix = matrix[:,boolean_array]\n",
    "    return matrix[boolean_array,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb8a6ad-d486-40bd-b2cb-6b3590af3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_matrix(matrix: np.array, list_index: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "        Remove sensors from the temporal series\n",
    "    \"\"\"\n",
    "    list_index2 = []\n",
    "    for i in range(len(list_index)):\n",
    "        if list_index[i] == 0:\n",
    "            list_index2.append(i)\n",
    "    return np.delete(matrix, list_index2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a36aec-21c8-497f-8251-f55e4fc212cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sensors_list(listt: np.array, list_index: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "        Remove sensors from a list\n",
    "    \"\"\"\n",
    "    boolean_array = list_index == 1\n",
    "    return listt[boolean_array]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
